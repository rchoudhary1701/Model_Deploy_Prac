# cloudbuild.yaml
# A complete CI/CD pipeline for the M&S Demand Forecasting model.

steps:
# Step 1: Install dependencies and run unit tests to ensure code quality.
- name: 'python:3.9'
  id: 'Run Tests'
  entrypoint: 'sh'
  args:
  - '-c'
  - |
    pip install -r requirements.txt
    pytest tests/

# Step 2: Build the Docker image, tagging it with the unique Git commit hash for traceability.
- name: 'gcr.io/cloud-builders/docker'
  id: 'Build Image'
  args: [
    'build',
    '-t',
    '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}:${SHORT_SHA}',
    '.'
    ]

# Step 3: Push the versioned container image to Google Artifact Registry.
- name: 'gcr.io/cloud-builders/docker'
  id: 'Push Image'
  args: [
    'push',
    '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}:${SHORT_SHA}'
    ]

# Step 4: Dynamically update the pipeline definition to use the new image URI.
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  id: 'Update Pipeline Spec'
  entrypoint: 'sh'
  args:
  - '-c'
  - |
    sed -i "s|DOCKER_IMAGE_URI = .*|DOCKER_IMAGE_URI = \\\"${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}:${SHORT_SHA}\\\"|" src/pipeline.py

# Step 5: Trigger the Vertex AI Pipeline using a Python script.
# This is a more robust method than using the gcloud CLI in Cloud Build.
- name: 'python:3.9'
  id: 'Trigger Pipeline'
  entrypoint: 'sh'
  args:
  - '-c'
  - |
    pip install google-cloud-aiplatform kfp --upgrade
    python3 src/pipeline.py --project_id=${PROJECT_ID} --bucket_name=${_BUCKET_NAME_SUB} --run

# This lists the final image URI in the build artifacts for reference.
images:
- '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPO_NAME}/${_IMAGE_NAME}:${SHORT_SHA}'

# These are substitution variables that Cloud Build will use.
substitutions:
  _REGION: 'europe-west1'
  _REPO_NAME: 'forecasting-repo'
  _IMAGE_NAME: 'demand-forecaster'
  _BUCKET_NAME_SUB: 'ms-forecast-bucket-372302732979' # IMPORTANT: Replace with your actual bucket name

# This option ensures logs are correctly streamed to Cloud Logging.
options:
  logging: CLOUD_LOGGING_ONLY

